{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d5c0ca",
   "metadata": {},
   "source": [
    "Dado que ya contamos con la clase encargada de conectarse a MLflow y obtener los modelos en producción, así como con la clase que supervisa los datos de entrada, es momento de construir la API.\n",
    "\n",
    "1. Se importan las librerías básicas necesarias para FastAPI.\n",
    "\n",
    "2. Se incorporan las clases previamente creadas: la encargada de la conexión con MLflow y aquella que define la estructura de los datos que el modelo requiere para realizar predicciones.\n",
    "\n",
    "3. Se configura el sistema de registros (logging), que permitirá monitorear el comportamiento de la aplicación durante su ejecución (alertas, errores, entre otros). Asimismo, se definen las variables globales que funcionarán como memoria caché: un diccionario de modelos (ml_models) y un diccionario de manejadores de servicios (service_handlers). Y se lee el archivo con las credenciales de minio para poder acceder a los modelos.\n",
    "\n",
    "4. Se establece el contexto de ciclo de vida (lifespan) para la aplicación FastAPI, lo que permite controlar las acciones que ocurren antes de que la aplicación comience a atender peticiones y después de que se apague. Para ello, se utiliza el decorador @asynccontextmanager, cuyo propósito es definir un bloque que gestiona recursos que requieren inicialización y liberación (setup / teardown) dentro de funciones asíncronas. La primera función async es lifespan(app: FastAPI), en la cual, al iniciar la aplicación, se ejecuta el siguiente servicio:\n",
    "\n",
    "    -  `service_handlers['mlflow'] = MLFlowHandler()`\n",
    "\n",
    "    Este servicio crea una única instancia de MLFlowHandler y la almacena en el diccionario global service_handlers, de modo que cualquier parte del código (por ejemplo, los endpoints de FastAPI) pueda acceder a ella sin necesidad de volver a inicializarla. Además, se registra en el log que el manejador ha sido inicializado, y se hace mediante la siguiente línea:\n",
    "\n",
    "    - `logging.info(\"Initialised mlflow handler {}\".format(type(service_handlers['mlflow'])))`\n",
    "\n",
    "    Posteriormente, tras el yield, cuando la aplicación se detiene, se limpian los diccionarios **service_handlers** y **ml_models** para liberar memoria y cerrar conexiones. Finalmente, se deja constancia en el log de que todo ha sido limpiado.Todo este proceso se hace con las siguientes líneas:\n",
    "\n",
    "    - yield\n",
    "    - service_handlers.clear()\n",
    "    - ml_models.clear()\n",
    "    - logging.info(\"Handlers y ml models limpiado\")\n",
    "\n",
    "5. Se crea la aplicación con FastAPI, en donde se le establece los siguientes parámetros:\n",
    "    - título: API de Evaluación de Riesgo Crediticio.\n",
    "    - descripción: Servicio que estima la probabilidad de incumplimiento de un préstamo dentro de los 12 meses posteriores a su adquisición.\n",
    "\n",
    "    Este aplicación se conecta al ciclo de vida de la aplicación mediante el código la linea:\n",
    "    - lifespan=lifespan\n",
    "\n",
    "6. Se define un `endpoint HTTP GET` en la aplicación FastAPI, disponible en la ruta:\n",
    "\n",
    "    - `http://<tu-servidor>/health/:` El propósito de este endpoint es reportar el estado de salud del sistema de machine learning, principalmente la conexión con MLflow y el estado del Model Registry.\n",
    "\n",
    "    Cuando una aplicación cliente (por ejemplo, postman) envía una solicitud a /health/, FastAPI ejecuta la función `async def healthcheck()`. Esa función hace tres llamadas importantes (todas a través del objeto service_handlers['mlflow'])\n",
    "\n",
    "    - `\"serviceStatus: \"OK\"\"`  Simplemente indica que el servidor FastAPI está funcionando y ha podido procesar la petición.\n",
    "\n",
    "    - `\"modelTrackingHealth\": service_handlers['mlflow'].check_mlflow_health()`  Comprueba si el servidor MLflow está disponible y responde correctamente. Si todo esta bien devuelve **\"Service returning experiments\"**.\n",
    "\n",
    "    - `\"modelRegistryHealth\": service_handlers['mlflow'].check_registry_health()`  Consulta el registro de modelos en MLflow para comprobar si existe al menos una versión de modelo en el estado de **\"Production\"**. Devuelve True si al menos un modelo está en producción.\n",
    "\n",
    "    - `\"productionModels\": service_handlers['mlflow'].list_production_models()`  Lista todos los modelos actualmente en etapa **“Production”** en el registro. Devuelve una lista de diccionarios con nombre del modelo, versión , descipción, etc.\n",
    "\n",
    "\n",
    "7. Se define un `endpoint HTTP GET` en la aplicación FastAPI, disponible en la ruta:\n",
    "\n",
    "    - `http://<tu-servidor>/debug/mlflow/:` El propósito de este endpoit es que cada vez que se haga una petición GET a esta url, FastAPI ejecuta la función **async def debug_mlflow()** y se llama al método `debug_registry()` mediante la linea *service_handlers['mlflow'].debug_registry()*. Este se encarga de obtener la información completa del estado actual del Model Registry de MLflow. Devolviendo así todos los modelos registrados y sus versiones más recientes incluyendo su nombre, descripción, estado, etapa (stage) y versiones que están actualmente en producción.\n",
    "\n",
    "8. Se define un `endpoint HTTP POST` el cual recibe los datos de entrada y detecta si la entrada proviene de un archivo CSV subido (UploadFile) o\n",
    "un cuerpo JSON, los transforma en un DataFrame válido mediante `ClassificationRequest`, y utiliza un modelo de `scikit-learn` cargado desde caché para generar predicciones de probabilidad por clase. Si el modelo no se encuentra en caché, se lanza un error HTTP 500.  En caso de error en el procesamiento de los datos o la inferencia, se devuelve un error HTTP 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías generales\n",
    "import os\n",
    "os.chdir(\"/media/luisgarcia/Datos/32. Ejercicio de acercamiento al rol/mlops_pyme/src/serve\")\n",
    "\n",
    "# =============================================\n",
    "# 1. IMPORTACIONES GENERALES (lo básico de FastAPI)\n",
    "# =============================================\n",
    "from fastapi import FastAPI,HTTPException,UploadFile,File,Request\n",
    "from contextlib import asynccontextmanager # General: para manejar ciclo de vida\n",
    "import logging                             # General: para obtener los log de la aplicación\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "# 2. Importar los ódigos creados para comunicarnos con MLflow y validar datos  de entrada\n",
    "# =============================================\n",
    "# Usemos las dos clases que creamos MlflowHandler y ClassificationRequest\n",
    "from helpers.schemas import ClassificationRequest\n",
    "from registry.mlflow.mlflow_handler import MlflowHandler\n",
    "\n",
    "# =============================================\n",
    "# 3. CONFIGURACIÓN INICIAL\n",
    "# =============================================\n",
    "# Crear un sistema de registro que permite rastrear lo que sucede en la aplicación mientras se ejecuta.\n",
    "log_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\" \n",
    "logging.basicConfig(format = log_format, level = logging.INFO)\n",
    "\n",
    "# configuremos las variables globales (caché)\n",
    "ml_models = {}\n",
    "service_handlers = {}\n",
    "\n",
    "# carga credenciales de minio\n",
    "project_secrets = next(p for p in Path.cwd().parents if (p / 'data').exists())\n",
    "credential_path = lambda file: os.path.join(project_secrets,'secrets',file)\n",
    "credential_path = credential_path('credentials_minio.json')\n",
    "with open(credential_path, 'r') as file:\n",
    "    credentials_minio = json.load(file)\n",
    "# =============================================\n",
    "# 4. CICLO DE VIDA DE LA APP\n",
    "# =============================================\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI): # primera función función que al iniciar la aplicación ejecuta los siguientes servicios\n",
    "    \"\"\"\n",
    "    Ciclo de vida de la aplicación FastAPI.\n",
    "\n",
    "    Inicializa los servicios necesarios al arrancar la aplicación, \n",
    "    incluyendo la conexión con el servidor MLflow y la carga del modelo \n",
    "    de producción en memoria caché.  \n",
    "    Al detener la aplicación, libera los recursos y limpia los servicios \n",
    "    registrados.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    app : FastAPI\n",
    "        Instancia principal de la aplicación FastAPI.\n",
    "\n",
    "    Process\n",
    "    --------\n",
    "    - Crea una instancia de `MlflowHandler` configurada con las credenciales de MinIO.\n",
    "    - Carga el modelo `DecisionTree_CreditRiskModel` desde la etapa *Production* \n",
    "      de MLflow y lo almacena en caché.\n",
    "    - Registra los eventos relevantes en el sistema de logs.\n",
    "    - Tras finalizar la ejecución de la aplicación, limpia las variables \n",
    "      globales `service_handlers` y `ml_models`.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    None\n",
    "        Control temporal devuelto al manejador de contexto de FastAPI \n",
    "        durante la ejecución de la aplicación.\n",
    "\n",
    "    Raises\n",
    "    -----------\n",
    "    Exception\n",
    "        Si ocurre un error al intentar cargar el modelo desde MLflow \n",
    "        durante la inicialización.\n",
    "    \"\"\"\n",
    "    service_handlers['mlflow'] = MlflowHandler(\n",
    "        tracking_uri=\"http://localhost:5000\",\n",
    "        s3_endpoint=credentials_minio[\"endpoint_url\"],\n",
    "        aws_access_key=credentials_minio[\"aws_access_key_id\"],\n",
    "        aws_secret_key=credentials_minio[\"aws_secret_access_key\"]\n",
    ")\n",
    "    logging.info(\"Inicializado MlflowHandler {}\".format(type(service_handlers['mlflow'])))\n",
    "    # cargar el modelo\n",
    "    model_name = \"DecisionTree_CreditRiskModel\"\n",
    "    try:\n",
    "        ml_models[model_name] = service_handlers['mlflow'].get_production_sklearn_model(model_name)\n",
    "        logging.info(f\"Modelo '{model_name}' cargado y almacenado en caché.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"No se pudo cargar el modelo '{model_name}' al iniciar: {e}\")\n",
    "    \n",
    "    yield\n",
    "    # lo que se ejecuta después de cerrar la aplicación\n",
    "    service_handlers.clear()\n",
    "    ml_models.clear()\n",
    "    logging.info(\"Servicios Handlers y ml models limpiados\")\n",
    "\n",
    "# =============================================\n",
    "# 5. CREAR LA APLICACIÓN FASTAPI\n",
    "# =============================================\n",
    "app = FastAPI(\n",
    "    title=\"API de Evaluación de Riesgo Crediticio\",\n",
    "    description=\"Servicio que estima la probabilidad de incumplimiento \" \\\n",
    "    \"de un préstamo dentro de los 12 meses\" \\\n",
    "    \"posteriores a su adquisición.\",\n",
    "    lifespan=lifespan  # Conectar el ciclo de vida\n",
    ")\n",
    "\n",
    "# =============================================\n",
    "# 6. DEFINIR EL ENDPOINT HTTP GET PARA COMPROBAR ESTADO DEL SERVIDOR MLFLOW\n",
    "# =============================================\n",
    "@app.get(\"/health/\", status_code=200)\n",
    "async def healthcheck():\n",
    "    \"\"\"\n",
    "    Verifica el estado general del servicio y del servidor de MLflow.\n",
    "\n",
    "    Este endpoint permite comprobar que la API está activa y que los \n",
    "    servicios asociados a MLflow (tanto el servidor de tracking como \n",
    "    el registro de modelos) están funcionando correctamente. \n",
    "    Además, devuelve la lista de modelos actualmente en la etapa \n",
    "    *Production* dentro del registro de MLflow.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con información sobre el estado de la API \n",
    "        y del entorno de MLflow, incluyendo:\n",
    "            - **serviceStatus** (`str`): Estado general del servicio (`\"OK\"` si está activo).\n",
    "            - **modelTrackingHealth** (`bool`): Estado del servidor de tracking de MLflow.\n",
    "            - **modelRegistryHealth** (`bool`): Estado del registro de modelos.\n",
    "            - **productionModels** (`list[str]`): Lista de modelos en la etapa *Production*.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"serviceStatus\": \"OK\",\n",
    "        \"modelTrackingHealth\": service_handlers['mlflow'].check_mlflow_health(),\n",
    "        \"modelRegistryHealth\": service_handlers['mlflow'].check_registry_health(),\n",
    "        \"productionModels\": service_handlers['mlflow'].list_production_models()\n",
    "    }\n",
    "# =============================================\n",
    "# 7. DEFINIR EL ENDPOINT HTTP GET PARA OBTENER INFORMACIÓN COMPLETA DEL SERVIDOR MLFLOW\n",
    "# =============================================\n",
    "@app.get(\"/debug/mlflow/\", status_code=200)\n",
    "async def debug_mlflow():\n",
    "    return service_handlers['mlflow'].debug_registry()\n",
    "\n",
    "# =============================================\n",
    "# 8. DEFINIR EL ENDPOINT HTTP POST PARA OBTENER LAS PREDICCIONES\n",
    "# =============================================\n",
    "@app.post(\"/classify/\", status_code=200)\n",
    "async def classify(file: UploadFile | None = File(None), request: Request = None):\n",
    "    \"\"\"\n",
    "    Endpoint de clasificación de riesgo crediticio.\n",
    "\n",
    "    Este endpoint recibe datos de entrada en formato CSV o JSON, los transforma en un \n",
    "    DataFrame válido mediante `ClassificationRequest`, y utiliza un modelo de \n",
    "    `scikit-learn` cargado desde caché para generar predicciones de probabilidad \n",
    "    por clase. \n",
    "\n",
    "    Si el modelo no se encuentra en caché, se lanza un error HTTP 500.  \n",
    "    En caso de error en el procesamiento de los datos o la inferencia, \n",
    "    se devuelve un error HTTP 400.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : UploadFile | None, opcional\n",
    "        Archivo CSV cargado por el usuario con las observaciones a clasificar.\n",
    "    request : Request, opcional\n",
    "        Objeto de solicitud HTTP, usado para detectar si el contenido proviene de un\n",
    "        archivo o de un cuerpo JSON.\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    dict\n",
    "        Un diccionario con la siguiente información:\n",
    "        - **n_observaciones**: número de registros clasificados.\n",
    "        - **clases**: lista con los nombres de las clases del modelo.\n",
    "        - **predicciones**: lista de diccionarios con las probabilidades por clase.\n",
    "        - **datos**: representación de los datos de entrada como diccionario de listas.\n",
    "        - **tiempo_inferencia_seg**: tiempo total de inferencia en segundos.\n",
    "\n",
    "    Raises\n",
    "    -----------\n",
    "    HTTPException\n",
    "        - 400: Si ocurre un error al procesar los datos o realizar la clasificación.\n",
    "        - 500: Si el modelo no se encuentra disponible en caché.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        if file:\n",
    "            if file.filename.endswith(\".csv\"):\n",
    "                content = await file.read()\n",
    "                decoded = content.decode(\"utf-8\")\n",
    "                df = ClassificationRequest.from_input(decoded)\n",
    "        \n",
    "        elif \"application/json\" in request.headers.get(\"content-type\", \"\"):\n",
    "            data = await request.json()\n",
    "            df = ClassificationRequest.from_input(data)\n",
    "\n",
    "        else:\n",
    "            return {\"error\": \"Formato no reconocido\"}\n",
    "        \n",
    "        \n",
    "        model_name = \"DecisionTree_CreditRiskModel\"\n",
    "        if model_name not in ml_models:\n",
    "            raise HTTPException(status_code=500, detail=\"Modelo no encontrado en caché\")\n",
    "        model = ml_models[model_name]\n",
    "\n",
    "        start = time.time()\n",
    "        probabilities = model.predict_proba(df)\n",
    "\n",
    "        classes = getattr(model, \"clase_\", [f\"Clase_{i}\" for i in range(probabilities.shape[1])])\n",
    "        results = [\n",
    "                {cls: float(prob) for cls, prob in zip(classes, row)}\n",
    "                for row in probabilities\n",
    "                ]\n",
    "        inference_time = round(time.time() - start, 4)\n",
    "        return {\n",
    "                \"n_observaciones\": len(df),\n",
    "                \"clases\": list(classes),\n",
    "                \"predicciones\": results,\n",
    "                \"datos\": df.to_dict(orient=\"list\"),\n",
    "                \"tiempo_inferencia_seg\": inference_time\n",
    "            }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Error en la clasificación: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
